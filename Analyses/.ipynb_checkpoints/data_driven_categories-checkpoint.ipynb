{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cdb63a-647a-4f26-9f45-de71163d3149",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcbaa0d1-8cd7-4ba8-897f-251868637e7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# HuggingFace LLM Client Setup\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=\"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\",  # your key\n",
    ")\n",
    "\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "temperature = 1\n",
    "top_p = 1\n",
    "max_tokens = 500\n",
    "n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e811d4d-ba2a-446e-8d55-8674217c0ddc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_long = pd.read_csv('../Data/data_long.csv')\n",
    "\n",
    "# Shuffle rows for diversity\n",
    "data_long_shuffled = data_long.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07abd23-5739-4707-921b-3ec14634d46d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert in cognitive psychology and reasoning analysis.\n",
    "\n",
    "A participant was solving the following reasoning problem:\n",
    "\n",
    "QUESTION:\n",
    "{question_text}\n",
    "\n",
    "The most obvious or intuitive (but incorrect) answer was: {lured_answer}\n",
    "The correct answer was: {correct_answer}\n",
    "\n",
    "Here is what the participant said when thinking aloud:\n",
    "\n",
    "TRANSCRIPTION:\n",
    "{transcription}\n",
    "\n",
    "Your task is to analyze the transcription and extract broad, generic deliberation functions that appear in the participant's reasoning process.\n",
    "\n",
    "A deliberation function refers to a distinct mental operation or reasoning strategy.\n",
    "\n",
    "Rules:\n",
    "- You should not refer to any specific content from the question or answers.\n",
    "- The functions should be generic and apply to reasoning in general.\n",
    "- The function can correspond to a small part of the transcription or the whole.\n",
    "- If no clear function is expressed, simply write \"No clear function identified.\"\n",
    "\n",
    "Output format:\n",
    "\n",
    "[List of functions]\n",
    "\n",
    "Each function should be a short NAME (1-3 words), followed by the specific part of the transcription that illustrates it.\n",
    "\n",
    "Number the functions only if there is more than one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df5c068-f9a6-4d1e-9c1e-78d4be5d6855",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_deliberation_functions(row):\n",
    "    prompt = prompt_template.format(\n",
    "        question_text=row['question_text'],\n",
    "        lured_answer=row['lured_answer'],\n",
    "        correct_answer=row['correct_answer'],\n",
    "        transcription=row['transcription_new']\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        n=n\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39692aab-705e-43a5-8071-2131ebaff320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from previous run. Already processed 869 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting functions:  78%|███████▊  | 800/1020 [00:00<00:00, 1073.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress after 870 total rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting functions:  87%|████████▋ | 889/1020 [00:21<00:04, 29.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress after 900 total rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting functions:  90%|█████████ | 921/1020 [01:01<00:19,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress after 930 total rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting functions:  94%|█████████▍| 959/1020 [01:37<00:35,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress after 960 total rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting functions:  97%|█████████▋| 990/1020 [02:22<00:28,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress after 990 total rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting functions: 100%|██████████| 1020/1020 [02:51<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress after 1020 total rows...\n",
      "Final save...\n",
      "All results saved to ../Output/llm_generated_deliberation_functions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = '../Output/llm_generated_deliberation_functions.csv'\n",
    "os.makedirs('../Output', exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Load Previous Results If Any\n",
    "# --------------------------------------------------------------------\n",
    "if os.path.exists(output_path):\n",
    "    existing_results = pd.read_csv(output_path)\n",
    "    processed_keys = set(zip(existing_results['subject_id'], existing_results['question']))\n",
    "    print(f\"Resuming from previous run. Already processed {len(existing_results)} rows.\")\n",
    "    results = existing_results.to_dict(orient='records')\n",
    "else:\n",
    "    processed_keys = set()\n",
    "    results = []\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# LLM Extraction Loop\n",
    "# --------------------------------------------------------------------\n",
    "for idx, row in tqdm(data_long_shuffled.iterrows(), total=len(data_long_shuffled), desc=\"Extracting functions\"):\n",
    "\n",
    "    key = (row['subject_id'], row['question'])\n",
    "\n",
    "    # Skip if already processed\n",
    "    if key in processed_keys:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        functions_text = get_deliberation_functions(row)\n",
    "\n",
    "        results.append({\n",
    "            'subject_id': row['subject_id'],\n",
    "            'question': row['question'],\n",
    "            'question_text': row['question_text'],\n",
    "            'lured_answer': row['lured_answer'],\n",
    "            'correct_answer': row['correct_answer'],\n",
    "            'transcription': row['transcription_new'],\n",
    "            'functions_extracted': functions_text\n",
    "        })\n",
    "\n",
    "        if len(results) % 30 == 0:\n",
    "            print(f\"Saving progress after {len(results)} total rows...\")\n",
    "            pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for idx {idx}: {e}\")\n",
    "        time.sleep(60)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Final Save\n",
    "# --------------------------------------------------------------------\n",
    "print(\"Final save...\")\n",
    "pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "print(f\"All results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52b5ebee-029a-49cb-9096-09e8650cb196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted functions: 2834\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Load LLM Generated Deliberation Functions\n",
    "# --------------------------------------------------------------------\n",
    "df_llm = pd.read_csv('../Output/llm_generated_deliberation_functions.csv')\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Extract Functions from functions_extracted Column\n",
    "# --------------------------------------------------------------------\n",
    "functions_list = []\n",
    "\n",
    "for idx, row in df_llm.iterrows():\n",
    "    functions_text = row['functions_extracted']\n",
    "\n",
    "    if pd.isna(functions_text):\n",
    "        continue  # skip missing\n",
    "\n",
    "    for block in functions_text.split(\"\\n\"):\n",
    "        if block.strip().lower().startswith('no clear function'):\n",
    "            continue  # skip this label\n",
    "        if not block.strip():\n",
    "            continue  # skip empty lines\n",
    "\n",
    "        # Remove numbering like \"1. \"\n",
    "        block = re.sub(r'^\\d+\\.\\s*', '', block)\n",
    "\n",
    "        # Split only on first ' - ' (function name vs excerpt)\n",
    "        if ' - ' in block:\n",
    "            func, excerpt = block.split(' - ', 1)\n",
    "        else:\n",
    "            func = block\n",
    "            excerpt = \"\"\n",
    "\n",
    "        functions_list.append({\n",
    "            'function': func.strip(),\n",
    "            'excerpt': excerpt.strip()\n",
    "        })\n",
    "\n",
    "print(f\"Total extracted functions: {len(functions_list)}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Create DataFrame with Extracted Functions\n",
    "# --------------------------------------------------------------------\n",
    "functions_df = pd.DataFrame(functions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91c8ae58-3601-4133-afca-7eb603101d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Initial Assumption — 90\n",
      "2. Initial Hypothesis — 69\n",
      "3. Assumption Making — 38\n",
      "4. Conclusion Drawing — 38\n",
      "5. Reevaluation — 38\n",
      "6. Question Reiteration — 34\n",
      "7. Hypothesis Generation — 32\n",
      "8. Confirmation — 32\n",
      "9. Hypothesis Formation — 30\n",
      "10. Knowledge Retrieval — 29\n",
      "11. Repeating Question — 28\n",
      "12. Evaluating Options — 27\n",
      "13. Premature Conclusion — 26\n",
      "14. Repeating Information — 26\n",
      "15. Elaboration — 26\n",
      "16. Repetition — 25\n",
      "17. Error Detection — 23\n",
      "18. Initial Response — 21\n",
      "19. Justification — 20\n",
      "20. Pattern Recognition — 19\n",
      "21. Default Assumption — 19\n",
      "22. Initial Repetition — 17\n",
      "23. Analogical Reasoning — 17\n",
      "24. Contextual Consideration — 17\n",
      "25. Information Retrieval — 16\n",
      "26. Initial Interpretation — 16\n",
      "27. Identifying Assumptions — 15\n",
      "28. Conclusion Formation — 14\n",
      "29. Initial Assessment — 14\n",
      "30. Alternative Consideration — 13\n",
      "31. Uncertainty Expression — 13\n",
      "32. Simplification — 13\n",
      "33. Error Recognition — 12\n",
      "34. Self-Correction — 12\n",
      "35. Considering Alternatives — 11\n",
      "36. Reiteration — 11\n",
      "37. Self-Questioning — 10\n",
      "38. Meta-Cognition — 10\n",
      "39. Drawing Conclusions — 10\n",
      "40. Initial Confusion — 10\n",
      "41. Initial Reading — 10\n",
      "42. Repeating Options — 10\n",
      "43. Inference Making — 9\n",
      "44. Pattern Completion — 9\n",
      "45. Alternative Generation — 9\n",
      "46. Insight Generation — 9\n",
      "47. Initial Guess — 9\n",
      "48. Contextual Analysis — 9\n",
      "49. Restating Problem — 9\n",
      "50. Correction — 9\n",
      "51. Misinterpretation — 9\n",
      "52. Clarification — 8\n",
      "53. Identifying Errors — 8\n",
      "54. Re-reading — 8\n",
      "55. Tentative Conclusion — 8\n",
      "56. Initial Consideration — 8\n",
      "57. Oversimplification — 8\n",
      "58. Pattern Application — 8\n",
      "59. Evaluation — 8\n",
      "60. Drawing Conclusion — 8\n",
      "61. Expression of Uncertainty — 7\n",
      "62. Hypothesis formation — 7\n",
      "63. Generating Examples — 7\n",
      "64. Consequence Evaluation — 7\n",
      "65. Alternative Evaluation — 7\n",
      "66. Weighing Options — 7\n",
      "67. Contextualizing — 7\n",
      "68. Reading Comprehension — 7\n",
      "69. Initial Reaction — 7\n",
      "70. Conclusion Revision — 7\n",
      "71. Suspicion of Deception — 7\n",
      "72. Elimination Strategy — 7\n",
      "73. Confirmation Bias — 7\n",
      "74. Expressing Uncertainty — 7\n",
      "75. Questioning Assumptions — 7\n",
      "76. Rejection — 7\n",
      "77. Seeking Clarification — 7\n",
      "78. Inference — 6\n",
      "79. Assumption Challenge — 6\n",
      "80. Information Reiteration — 6\n",
      "81. Reality Checking — 6\n",
      "82. Contextual Reevaluation — 6\n",
      "83. Corrective Insight — 6\n",
      "84. Logical Inference — 6\n",
      "85. Identifying Prerequisites — 6\n",
      "86. Eliminating Choices — 6\n",
      "87. Doubt Expression — 6\n",
      "88. Evaluating Consequences — 6\n",
      "89. Correction Application — 6\n",
      "90. Reality Check — 6\n",
      "91. Emotional Reaction — 6\n",
      "92. Pattern assumption — 6\n",
      "93. Restatement — 6\n",
      "94. Scenario Elaboration — 5\n",
      "95. Reading Aloud — 5\n",
      "96. Information Repetition — 5\n",
      "97. Information Integration — 5\n",
      "98. Inference Generation — 5\n",
      "99. Making Assumptions — 5\n",
      "100. Default Reasoning — 5\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "Top 100 Most Frequent Functions\n",
    "# --------------------------------------------------------------------\n",
    "from collections import Counter\n",
    "\n",
    "# Extract only the function names\n",
    "func_names = [item['function'] for item in functions_list]\n",
    "\n",
    "# Count occurrences\n",
    "func_counts = Counter(func_names)\n",
    "\n",
    "# Put in a DataFrame\n",
    "func_freq_df = pd.DataFrame(func_counts.most_common(100), columns=['function', 'count'])\n",
    "\n",
    "for idx, row in func_freq_df.iterrows():\n",
    "    print(f\"{idx+1}. {row['function']} — {row['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c70758b2-c538-49f9-a5cb-11fa4851b669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Prepare Prompt for Classifying Subfunctions into Core Deliberation Functions\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in cognitive psychology and reasoning research.\n",
    "\n",
    "You are provided with a list of fine-grained deliberation subfunctions that have been automatically extracted from participants' transcriptions during reasoning tasks.\n",
    "\n",
    "Your task is to classify each subfunction into one of the following 6 broad deliberation functions:\n",
    "\n",
    "1. Response Control: Inhibiting or rejecting an intuitive or prepotent response.\n",
    "2. Response Generation: Actively generating new possible answers, alternatives, or hypotheses.\n",
    "3. Response Justification: Providing reasons, arguments, or explanations to support a response.\n",
    "4. Response Regulation: Reflecting on the reasoning process, monitoring uncertainty, evaluating effort, or deciding how to proceed.\n",
    "5. Other: Use this category for subfunctions that are not directly linked to reasoning or deliberation.\n",
    "6. Non-Assignable: Use this category if the subfunction is too ambiguous, unclear, or context-dependent to be confidently assigned to one of the above functions.\n",
    "\n",
    "Guidelines:\n",
    "- Assign each subfunction to exactly one of the categories above.\n",
    "- Be theoretically rigorous: base your decisions on the typical role of the subfunction during reasoning.\n",
    "- Avoid overly broad categorizations:\n",
    "    - Explanation or defending an answer → Response Justification\n",
    "    - Exploring new options → Response Generation\n",
    "    - Reflecting on effort or confidence → Response Regulation\n",
    "    - Inhibiting first thoughts → Response Control\n",
    "- If in doubt, prefer using the Non-Assignable category.\n",
    "\n",
    "Output Format:\n",
    "Provide your output exactly as follows:\n",
    "\n",
    "# Response Control\n",
    "- Subfunction A\n",
    "- Subfunction B\n",
    "...\n",
    "\n",
    "# Response Generation\n",
    "- Subfunction C\n",
    "...\n",
    "\n",
    "# Response Justification\n",
    "- Subfunction D\n",
    "...\n",
    "\n",
    "# Response Regulation\n",
    "- Subfunction E\n",
    "...\n",
    "\n",
    "# Other\n",
    "- Subfunction F\n",
    "...\n",
    "\n",
    "# Non-Assignable\n",
    "- Subfunction G\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Content Prompt (Dynamic from your Top Functions)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "content_prompt = f\"\"\"Here is the list of subfunctions to classify:\n",
    "\n",
    "{chr(10).join(f\"- {f}\" for f in top_functions)}\n",
    "\n",
    "Classify them into the 6 categories as instructed.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69e7761f-fa38-43b3-9f5b-e33cb1aa6473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM call complete.\n",
      "Taxonomy saved to ../Output/llm_generated_deliberation_taxonomy.txt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 6. LLM Call to Generate Data-Driven Broad Deliberation Functions\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "\n",
    "# Perform the call\n",
    "try:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": content_prompt}\n",
    "    ]\n",
    "\n",
    "    taxonomy_completion = client.chat.completions.create(\n",
    "        model=LLAMA_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=2000,  # Larger token size for long output\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    taxonomy_text = taxonomy_completion.choices[0].message.content\n",
    "\n",
    "    print(\"LLM call complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during LLM call: {e}\")\n",
    "    time.sleep(60)\n",
    "    taxonomy_text = None\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7. Save Taxonomy to Output\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "output_path = \"../Output/llm_generated_deliberation_taxonomy.txt\"\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(taxonomy_text)\n",
    "\n",
    "print(f\"Taxonomy saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
