---
title: "Trajectory Analysis LLM"
author: "Jérémie Beucler"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# remove all objects
rm(list = ls())

# --------------------------------------------------------------------
# Libraries
# --------------------------------------------------------------------

library(zoo)
library(here)
library(ggplot2)
library(ggthemes)
library(data.table)
library(ggtext) # For markdown in titles
library(patchwork) # To combine plots
library(scales)    # Better axis formatting
library(viridis)   # Color palette
library(jtools)
library(mgcv)
library(gratia)  # for appraise(), difference_smooth(), etc.
library(itsadug) # for plot_diff(), concurrency checks, etc.
library(tidyverse)

# --------------------------------------------------------------------
# Load Data
# --------------------------------------------------------------------
# Load scored chunks
chunks_scored <- read.csv("./Output/scored_chunks.csv")

# Load full data_long (with response etc.)
data_long <- read.csv("./Data/data_long.csv")

# --------------------------------------------------------------------
# Define Custom Colors for Functions
# --------------------------------------------------------------------
function_colors <- c(
  "Control" = "firebrick2",        # Red
  "Generation" = "royalblue1",     # Blue
  "Justification" = "gold2",  # Orange
  "Regulation" = "forestgreen"      # Green
)

# --------------------------------------------------------------------
# Merge additional variables
# --------------------------------------------------------------------
chunks_scored <- chunks_scored %>%
  left_join(
    data_long %>% dplyr::select(subject_id, question, response, reconsidered_initial_resp, verbalized_reasons, lure_consideration, familiar) %>% distinct(),
    by = c("subject_id", "question")
  )

# --------------------------------------------------------------------
# Compute Number of Words in Transcription_new
# --------------------------------------------------------------------
transcription_lengths <- data_long %>%
  select(subject_id, question, transcription_new) %>%
  mutate(
    n_words = str_count(transcription_new, "\\S+")
  )

# --------------------------------------------------------------------
# Merge Word Count to Chunks Scored
# --------------------------------------------------------------------
chunks_scored <- chunks_scored %>%
  left_join(transcription_lengths %>% select(subject_id, question, n_words),
            by = c("subject_id", "question"))

# --------------------------------------------------------------------
# Filter to Keep Only Transcriptions with Reflect or Lure
# --------------------------------------------------------------------
chunks_scored <- chunks_scored %>%
  filter(response == "Reflect" | response == "Lure")

# filter out responses where max number of chunks is 1
chunks_scored <- chunks_scored %>%
  group_by(subject_id, question) %>%
  filter(max(chunk_id) > 1) %>%
  ungroup()

# --------------------------------------------------------------------
# Check
# --------------------------------------------------------------------
summary(chunks_scored$n_words)
print(dim(chunks_scored))
```

## Correlation between Deliberation Functions

```{r}
# --------------------------------------------------------------------
# Libraries for Correlation Heatmap
# --------------------------------------------------------------------
library(ggcorrplot)  # For pretty correlation plots
library(Hmisc)       # For rcorr()

# --------------------------------------------------------------------
# Select Variables
# --------------------------------------------------------------------
cor_vars <- chunks_scored %>%
  select(response_control, response_generation, response_justification, response_regulation)

# --------------------------------------------------------------------
# Compute Correlation Matrix & p-values
# --------------------------------------------------------------------
cor_res <- Hmisc::rcorr(as.matrix(cor_vars), type = "pearson")

cor_matrix <- cor_res$r
p_matrix <- cor_res$P

# --------------------------------------------------------------------
# Plot Correlation Heatmap
# --------------------------------------------------------------------
ggcorrplot(cor_matrix, 
           type = "upper", 
           p.mat = p_matrix, 
           insig = "blank",        # Hide insignificant values
           lab = TRUE,             # Show correlation values
           lab_size = 4, 
           colors = c("darkblue", "white", "darkred"),
           outline.color = "grey50",
           title = "Correlation between Deliberation Functions",
           legend.title = "r") +
  theme_apa() +
  theme(plot.title = element_text(face = "bold"))

ggsave("./Output/deliberation_function_correlation_heatmap.png", dpi = 600, width = 8, height = 5)

```

## Validation with handscored data

```{r}
# --------------------------------------------------------------------
# Average Function Scores per Subject and Question
# --------------------------------------------------------------------
avg_scores <- chunks_scored %>%
  group_by(subject_id, question) %>%
  summarise(
    control = mean(response_control, na.rm = TRUE),
    generation = mean(response_generation, na.rm = TRUE),
    justification = mean(response_justification, na.rm = TRUE),
    regulation = mean(response_regulation, na.rm = TRUE),
    reconsidered_initial_resp = unique(reconsidered_initial_resp),
    verbalized_reasons = unique(verbalized_reasons)
  ) %>%
  ungroup()

# --------------------------------------------------------------------
# Filter to Binary Values Only
# --------------------------------------------------------------------
avg_scores_filtered <- avg_scores %>%
  filter(reconsidered_initial_resp %in% c(0, 1),
         verbalized_reasons %in% c(0, 1))

# --------------------------------------------------------------------
# Select Variables for Correlation
# --------------------------------------------------------------------
cor_vars_handscored <- avg_scores_filtered %>%
  select(control, generation, justification, regulation,
         reconsidered_initial_resp, verbalized_reasons)

# --------------------------------------------------------------------
# Compute Correlation Matrix with p-values
# --------------------------------------------------------------------
cor_res_handscored <- Hmisc::rcorr(as.matrix(cor_vars_handscored), type = "pearson")

cor_matrix_handscored <- cor_res_handscored$r
p_matrix_handscored <- cor_res_handscored$P

# --------------------------------------------------------------------
# Plot Correlation Heatmap
# --------------------------------------------------------------------
ggcorrplot(cor_matrix_handscored, 
           type = "upper", 
           p.mat = p_matrix_handscored, 
           insig = "blank",        
           lab = TRUE,             
           lab_size = 4, 
           colors = c("darkblue", "white", "darkred"),
           outline.color = "grey50",
           title = "Correlation Between LLM Scores and Handscored Variables",
           legend.title = "r") +
  theme_apa() +
  theme(plot.title = element_text(face = "bold"))

ggsave("./Output/deliberation_function_handscored_variables_correlation_heatmap_handscored.png", dpi = 600, width = 8, height = 5)
```




```{r}
# --------------------------------------------------------------------
# Compute Normalized Position per Chunk
# --------------------------------------------------------------------
chunks_scored <- chunks_scored %>%
  group_by(subject_id, question) %>%
  arrange(chunk_id) %>%
  mutate(
    n_chunks = n(),
    normalized_position = ifelse(n_chunks == 1, 0, (chunk_id - 1) / (n_chunks - 1))
  ) %>%
  ungroup()

# --------------------------------------------------------------------
# Pivot Longer for Functions
# --------------------------------------------------------------------
chunks_long <- chunks_scored %>%
  pivot_longer(
    cols = c(response_control, response_generation, response_justification, response_regulation),
    names_to = "deliberation_function",
    values_to = "score"
  )

chunks_long = chunks_long %>%
  mutate(deliberation_function = recode(deliberation_function,
                                        response_control = "Control",
                                        response_generation = "Generation",
                                        response_justification = "Justification",
                                        response_regulation = "Regulation"),
         response = ifelse(response == "Reflect", "Unbiased Response", "Biased Response"))


# --------------------------------------------------------------------
# Aggregate Mean Trajectory per Function
# --------------------------------------------------------------------
mean_trajectory_norm <- chunks_long %>%
  group_by(deliberation_function, normalized_position) %>%
  summarise(score = mean(score, na.rm = TRUE)) %>%
  ungroup()

# --------------------------------------------------------------------
# Smoothing Function
# --------------------------------------------------------------------
smooth_trajectory <- function(df, window = 1) {
  df %>%
    group_by(deliberation_function) %>%
    arrange(normalized_position) %>%
    mutate(score = rollmean(score, k = window, fill = NA, align = "center")) %>%
    drop_na()
}
```

```{r}
# --------------------------------------------------------------------
# Compute Number of Chunks per Question
# --------------------------------------------------------------------
n_chunks_df <- chunks_scored %>%
  group_by(subject_id, question, response) %>%
  summarise(n_chunks = max(chunk_id)) %>%
  ungroup()

# --------------------------------------------------------------------
# Summary Statistics (Overall)
# --------------------------------------------------------------------
print("Summary Statistics for Number of Chunks per Question (Overall):")
summary(n_chunks_df$n_chunks)

# --------------------------------------------------------------------
# Summary Statistics by Response
# --------------------------------------------------------------------
print("Summary Statistics for Number of Chunks per Question by Response:")

n_chunks_df %>%
  group_by(response) %>%
  summarise(
    count = n(),
    mean = mean(n_chunks),
    sd = sd(n_chunks),
    min = min(n_chunks),
    p25 = quantile(n_chunks, 0.25),
    median = median(n_chunks),
    p75 = quantile(n_chunks, 0.75),
    max = max(n_chunks)
  )

# --------------------------------------------------------------------
# Histogram of Number of Chunks (Overall)
# --------------------------------------------------------------------
ggplot(n_chunks_df, aes(x = n_chunks)) +
  geom_histogram(bins = 15, fill = "#1f77b4", color = "white", alpha = 0.8) +
  geom_density(aes(y = ..count..), color = "black", linewidth = 1) +
  labs(
    x = "Number of Chunks per Question",
    y = "Count",
    title = "Distribution of Number of Chunks per Question"
  ) +
  theme_apa()

ggsave("./Output/histogram_n_chunks_overall.png", dpi = 600, width = 8, height = 5)

# --------------------------------------------------------------------
# Histogram of Number of Chunks (Grouped by Response)
# --------------------------------------------------------------------
ggplot(n_chunks_df, aes(x = n_chunks, fill = response)) +
  geom_histogram(bins = 15, alpha = 0.7, position = "identity", color = "white") +
  facet_wrap(~response, ncol = 2) +
  labs(
    x = "Number of Chunks per Question",
    y = "Count",
    title = "Distribution of Number of Chunks per Question by Response"
  ) +
  theme_apa()

ggsave("./Output/histogram_n_chunks_by_response.png", dpi = 600, width = 10, height = 6)

n_chunks_df_lure = chunks_scored %>%
  group_by(subject_id, question, lure_consideration) %>%
  summarise(n_chunks = max(chunk_id)) %>%
  ungroup()

# now by lure consideration
ggplot(n_chunks_df_lure, aes(x = n_chunks, fill = factor(lure_consideration))) +
  geom_histogram(bins = 15, alpha = 0.7, position = "identity", color = "white") +
  facet_wrap(~lure_consideration, ncol = 2) +
  labs(
    x = "Number of Chunks per Question",
    y = "Count",
    title = "Distribution of Number of Chunks per Question by Lure Consideration"
  ) +
  theme_apa()

ggsave("./Output/histogram_n_chunks_by_lure_consideration.png", dpi = 600, width = 10, height = 6)

# now response and lure consideration
n_chunks_df_lure_resp = chunks_scored %>%
  group_by(subject_id, question, response, lure_consideration) %>%
  summarise(n_chunks = max(chunk_id)) %>%
  ungroup()

ggplot(n_chunks_df_lure_resp, aes(x = n_chunks, fill = factor(lure_consideration))) +
  geom_histogram(bins = 15, alpha = 0.7, position = "identity", color = "white") +
  facet_grid(
    rows = vars(response),
    cols = vars(lure_consideration),
    labeller = label_both
  ) +
  labs(
    x = "Number of Chunks per Question",
    y = "Count",
    title = "Distribution of Number of Chunks per Question by Response and Lure Consideration"
  ) +
  theme_apa()

ggsave("./Output/histogram_n_chunks_by_response_lure_consideration.png", dpi = 600, width = 10, height = 6)



# Define the function names once
functions <- c("Control", "Generation", "Justification", "Regulation")

chunks_dominant <- chunks_scored %>%
  rowwise() %>%
  mutate(
    # Store scores as a list (vector inside a list)
    scores = list(c_across(c(response_control, response_generation, response_justification, response_regulation))),
    
    max_score = max(unlist(scores)),
    n_max = sum(unlist(scores) == max_score),
    
    # Only assign dominant function if no tie
    dominant_function = ifelse(
      n_max == 1,
      functions[which.max(unlist(scores))],
      NA_character_
    )
  ) %>%
  ungroup() %>%
  filter(!is.na(dominant_function)) %>%
  mutate(
    word_count = str_count(chunk_text, "\\S+"),
    char_count = nchar(chunk_text)
  )

# Boxplot of word count with mean
ggplot(chunks_dominant, aes(x = dominant_function, y = word_count, fill = dominant_function)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 5, color = "yellow") +
  labs(title = "Word Count per Chunk by Dominant Function",
       x = "Dominant Function",
       y = "Word Count") +
  theme_apa() +
  theme(legend.position = "none")

# Boxplot of character count with mean
ggplot(chunks_dominant, aes(x = dominant_function, y = char_count, fill = dominant_function)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 5, color = "yellow") +
  labs(title = "Character Count per Chunk by Dominant Function",
       x = "Dominant Function",
       y = "Character Count") +
  theme_apa()+
  theme(legend.position = "none")
```

```{r}
###
# 1) Copy data & initialize columns
###

chunks_scores_loop <- chunks_scored

# Create distance columns
chunks_scores_loop$control_dist <- NA
chunks_scores_loop$generation_dist <- NA
chunks_scores_loop$justification_dist <- NA
chunks_scores_loop$regulation_dist <- NA

n_rows <- nrow(chunks_scores_loop)

###
# 2) Hardcore for-loop: compute distance from the (100, 0, 0, 0)-style ideal
#    for each function in every row
###

for (i in seq_len(n_rows)) {
  
  # Extract row as a plain R vector
  row <- chunks_scores_loop[i, ]
  
  # Scores for convenience
  sc <- row$response_control
  sg <- row$response_generation
  sj <- row$response_justification
  sr <- row$response_regulation
  
  # Control => Ideal is (100, 0, 0, 0)
  # We'll store the Euclidean distance (you can skip sqrt() if you prefer squared distance)
  chunks_scores_loop$control_dist[i] <-
    sqrt((sc - 100)^2 + (sg - 0)^2 + (sj - 0)^2 + (sr - 0)^2)
  
  # Generation => Ideal is (0, 100, 0, 0)
  chunks_scores_loop$generation_dist[i] <-
    sqrt((sc - 0)^2 + (sg - 100)^2 + (sj - 0)^2 + (sr - 0)^2)
  
  # Justification => Ideal is (0, 0, 100, 0)
  chunks_scores_loop$justification_dist[i] <-
    sqrt((sc - 0)^2 + (sg - 0)^2 + (sj - 100)^2 + (sr - 0)^2)
  
  # Regulation => Ideal is (0, 0, 0, 100)
  chunks_scores_loop$regulation_dist[i] <-
    sqrt((sc - 0)^2 + (sg - 0)^2 + (sj - 0)^2 + (sr - 100)^2)
}

###
# 3) Pick the top 3 "pure" chunks per function — i.e., the ones with the LOWEST distance
###

# Prepare container
representative_chunks <- tibble()

function_labels <- c("Control", "Generation", "Justification", "Regulation")
dist_cols       <- c("control_dist", "generation_dist", "justification_dist", "regulation_dist")

# For each function, find the 3 best matches
for (idx in seq_along(function_labels)) {
  
  f_label   <- function_labels[idx]  # "Control", ...
  dist_var  <- dist_cols[idx]       # "control_dist", ...
  
  # Subset, sort by that dist ascending
  best_rows <- chunks_scores_loop %>%
    arrange(.data[[dist_var]]) %>%
    slice_head(n = 3) %>%
    mutate(Max_representative = f_label)
  
  # Combine
  representative_chunks <- bind_rows(representative_chunks, best_rows)
}

###
# 4) Merge with question info & reorder columns
###

representative_chunks_annotated <- representative_chunks %>%
  left_join(
    data_long %>%
      select(subject_id, question, question_text, correct_answer, lured_answer),
    by = c("subject_id", "question")
  ) %>%
  select(
    Max_representative,
    chunk_text,
    question_text,
    correct_answer,
    lured_answer,
    # Original 4 function scores
    response_control,
    response_generation,
    response_justification,
    response_regulation,
    # Distances
    control_dist,
    generation_dist,
    justification_dist,
    regulation_dist
  ) %>% 
  rename(Function = Max_representative,
         Chunk = chunk_text,
         Question = question_text,
         Correct_answer = correct_answer,
         Lured_answer = lured_answer,
         Control = response_control,
         Generation = response_generation,
         Justification = response_justification,
         Regulation = response_regulation) %>%
  select(-c(control_dist, generation_dist, justification_dist, regulation_dist))

write.csv(representative_chunks_annotated, "./Output/representative_chunks.csv", row.names = FALSE)
```




```{r}
# --------------------------------------------------------------------
# Plot Mean Trajectory (All Functions Together)
# --------------------------------------------------------------------
smoothed_norm <- smooth_trajectory(mean_trajectory_norm, window = 1)

ggplot(smoothed_norm, aes(x = normalized_position, y = score, color = deliberation_function)) +
  #geom_line(size = 1, alpha = 0.3) +
  geom_smooth(method = "loess", size = 1, se = FALSE, span = 1) +
  theme_apa() +
  scale_color_manual(values = function_colors) +
  labs(x = "Normalized Position in Text",
       y = "Mean Score",
       color = "Deliberation Function")

ggsave("./Output/overall_trajectory_loess_preliminary.png", dpi = 600, width = 12, height = 6)
```


```{r}
# --------------------------------------------------------------------
# Aggregate Mean Trajectory per Normalized Position and Response
# --------------------------------------------------------------------
mean_trajectory_norm_resp <- chunks_long %>%
  group_by(response, deliberation_function, normalized_position) %>%
  summarise(score = mean(score, na.rm = TRUE)) %>%
  ungroup()

# --------------------------------------------------------------------
# Smoothing Function
# --------------------------------------------------------------------
smooth_trajectory_response <- function(df, window = 1) {
  df %>%
    group_by(response, deliberation_function) %>%
    arrange(normalized_position) %>%
    mutate(score = zoo::rollmean(score, k = window, fill = NA, align = "center")) %>%
    drop_na()
}

# --------------------------------------------------------------------
# Apply Smoothing
# --------------------------------------------------------------------
smoothed_resp <- smooth_trajectory_response(mean_trajectory_norm_resp, window = 1)
```

```{r}
ggplot(smoothed_resp, aes(x = normalized_position, y = score, color = deliberation_function)) +
  #geom_line(size = 1, alpha = 0.3) +
  geom_smooth(method = "loess", size = 1, se = FALSE, span = 1) + # 2 rows, 1 column
  facet_wrap(~response, scales = "free_y", ncol = 1) +
  theme_apa() +
  scale_color_manual(values = function_colors) +
  labs(x = "Normalized Position in Text",
       y = "Mean Score",
       color = "Deliberation Function")

ggsave("./Output/trajectory_by_response_loess_preliminary.png", dpi = 600, width = 12, height = 10)
```



```{r}
# --------------------------------------------------------------------
# Compute Mean Trajectory by Response
# --------------------------------------------------------------------
mean_trajectory_resp <- chunks_long %>%
  group_by(response, deliberation_function, normalized_position) %>%
  summarise(score = mean(score, na.rm = TRUE)) %>%
  ungroup()

# --------------------------------------------------------------------
# Pivot Wider: Separate Unbiased and Biased
# --------------------------------------------------------------------
trajectory_diff <- mean_trajectory_resp %>%
  filter(response %in% c("Biased Response", "Unbiased Response")) %>%
  pivot_wider(
    names_from = response,
    values_from = score
  ) %>%
  mutate(difference = `Unbiased Response` - `Biased Response`)

# --------------------------------------------------------------------
# Smoothing Function
# --------------------------------------------------------------------
smooth_trajectory_diff <- function(df, window = 1) {
  df %>%
    group_by(deliberation_function) %>%
    arrange(normalized_position) %>%
    mutate(difference = zoo::rollmean(difference, k = window, fill = NA, align = "center")) %>%
    drop_na()
}

# --------------------------------------------------------------------
# Apply Smoothing
# --------------------------------------------------------------------
smoothed_diff <- smooth_trajectory_diff(trajectory_diff, window = 1)
```


```{r}
# --------------------------------------------------------------------
# Plot Difference Trajectories with Annotations
# --------------------------------------------------------------------
p_diff <- ggplot(smoothed_diff, aes(x = normalized_position, y = difference, color = deliberation_function)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  #geom_line(size = 1, alpha = 0.3) +
  geom_smooth(method = "loess", size = 1.2, se = FALSE, span = 1) +
  scale_color_manual(values = function_colors) +
  annotate("text", x = 0.05, y = max(smoothed_diff$difference, na.rm = TRUE) * 0.93, 
           label = "Higher for Unbiased", hjust = 0, size = 3) +
  annotate("text", x = 0.05, y = min(smoothed_diff$difference, na.rm = TRUE) * 0.93, 
           label = "Higher for Biased", hjust = 0, size = 3) +
  theme_apa() +
  coord_cartesian(ylim = c(-25, 30)) +
  labs(
    x = "Normalized Position in Text",
    y = "Difference in Mean Score (Unbiased - Biased)",
    color = "Deliberation Function"
  )

p_diff

#ggsave("./Output/difference_trajectory_unbiased_vs_biased_loess_preliminary.png", p_diff, dpi = 600, width = 10, height = 6)
```

```{r}
# --------------------------------------------------------------------
# Plot Difference Trajectories by Function (Separate Facets)
# --------------------------------------------------------------------
p_diff_facets <- ggplot(smoothed_diff, aes(x = normalized_position, y = difference, color = deliberation_function)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  #geom_line(size = 1, alpha = 0.3) +
  geom_smooth(method = "loess", size = 1.2, se = FALSE, span = 1) +
  facet_wrap(~deliberation_function, ncol = 2, scales = "free_y") +
  scale_color_manual(values = function_colors, guide = "none") +
  # annotate("text", x = 0.05, y = max(smoothed_diff$difference, na.rm = TRUE) * 0.93, 
  #          label = "Higher for Unbiased", hjust = 0, size = 3) +
  # annotate("text", x = 0.05, y = min(smoothed_diff$difference, na.rm = TRUE) * 0.93, 
  #          label = "Higher for Biased", hjust = 0, size = 3) +
  theme_apa() +
  labs(
    x = "Normalized Position in Text",
    y = "Difference in Mean Score (Unbiased - Biased)"
  )

p_diff_facets

ggsave("./Output/difference_trajectory_unbiased_biased_faceted_loess_preliminary.png", p_diff_facets, dpi = 600, width = 12, height = 8)
```

## GAM


```{r}
chunks_long <- chunks_long %>%
  mutate(
    deliberation_function = factor(deliberation_function),
    response              = factor(response),
    subject_id            = factor(subject_id),
    question              = factor(question),
    score
  )

# ----------------------------------------------------------------------------
# Fit GAM Model without Response
# ----------------------------------------------------------------------------
gam_model <- bam(
  score ~ deliberation_function + 
    s(normalized_position, by = deliberation_function, k = 10) +
    s(subject_id, bs = "re") +
    s(question, bs = "re"),
  data = chunks_long,
  method = "fREML"
)

summary(gam_model)
gam.check(gam_model)
appraise(gam_model)
```


```{r}
# ----------------------------------------------------------------------------
# Generate New Data for Predictions
# ----------------------------------------------------------------------------
newdata <- expand.grid(
  normalized_position = seq(0, 1, length.out = 100),
  deliberation_function = levels(chunks_long$deliberation_function)
)

newdata$subject_id <- levels(chunks_long$subject_id)[1]
newdata$question   <- levels(chunks_long$question)[1]

predictions <- predict(
  gam_model,
  newdata = newdata,
  se.fit = TRUE,
  exclude = c("s(subject_id)", "s(question)")
)

newdata$pred <- predictions$fit
newdata$se   <- predictions$se.fit

# ----------------------------------------------------------------------------
# Compute Observed Means for Overlay
# ----------------------------------------------------------------------------
observed_means <- chunks_long %>%
  group_by(deliberation_function, normalized_position) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    se_score   = sd(score, na.rm = TRUE) / sqrt(n())
  ) %>%
  ungroup()

# ----------------------------------------------------------------------------
# Plot: Trajectories per Deliberation Function
# ----------------------------------------------------------------------------

p_final <- ggplot() +
  
  # # Observed Means
  # geom_line(data = observed_means,
  #           aes(x = normalized_position, y = mean_score, color = deliberation_function),
  #           size = 0.8) +
  
  # GAM Fit Lines
  geom_line(data = newdata,
            aes(x = normalized_position, y = pred, color = deliberation_function),
            size = 1) +
  
  # GAM CI
  geom_ribbon(data = newdata,
              aes(x = normalized_position, ymin = pred - 1.96 * se, ymax = pred + 1.96 * se, fill = deliberation_function),
              alpha = 0.2, color = NA) +
  
  facet_wrap(~deliberation_function, ncol = 2, scales = "free_y") +
  
  theme_apa() +
  
  labs(x = "Normalized Position in Verbalization",
       y = "Predicted Score",
       color = "Deliberation Function",
       fill  = "Deliberation Function")

p_final

# ----------------------------------------------------------------------------
# Save Plot
# ----------------------------------------------------------------------------
ggsave("./Output/gam_trajectory_functions_overall_trajectory.png",
       p_final, dpi = 600, width = 12, height = 8)
```

```{r}
# ----------------------------------------------------------------------------
# Step 1: Generate Predictions Per Deliberation Function
# ----------------------------------------------------------------------------

# Desired subtraction order
delib_functions <- c("Generation", "Justification", "Control", "Regulation")

# Create newdata for predictions
newdata <- expand.grid(
  normalized_position = seq(0, 1, length.out = 100),
  deliberation_function = delib_functions
)

newdata$subject_id <- levels(chunks_long$subject_id)[1]
newdata$question   <- levels(chunks_long$question)[1]

predictions <- predict(
  gam_model,
  newdata = newdata,
  se.fit = TRUE,
  exclude = c("s(subject_id)", "s(question)")
)

newdata$pred <- predictions$fit
newdata$se   <- predictions$se.fit

# ----------------------------------------------------------------------------
# Step 2: Compute Manual Pairwise Differences
# ----------------------------------------------------------------------------

pairs <- combn(delib_functions, 2, simplify = FALSE)

diff_dfs <- list()

for(pair in pairs) {
  
  func1 <- pair[1]  # Always subtract in this fixed order
  func2 <- pair[2]
  
  df1 <- newdata %>% filter(deliberation_function == func1)
  df2 <- newdata %>% filter(deliberation_function == func2)
  
  diff_df <- df1 %>%
    mutate(
      deliberation_function_1 = func1,
      deliberation_function_2 = func2,
      diff = df1$pred - df2$pred,   # Always func1 - func2
      se_diff = sqrt(df1$se^2 + df2$se^2),
      zval = diff / se_diff,
      pval = 2 * (1 - pnorm(abs(zval))),
      lower_ci = diff - 1.96 * se_diff,
      upper_ci = diff + 1.96 * se_diff
    )
  
  diff_dfs[[paste(func1, func2, sep = "_vs_")]] <- diff_df
}

diff_all <- bind_rows(diff_dfs)

# ----------------------------------------------------------------------------
# Step 3: Correct for Multiple Comparisons (FDR)
# ----------------------------------------------------------------------------

diff_all <- diff_all %>%
  group_by(deliberation_function_1, deliberation_function_2) %>%
  mutate(pval_adj = p.adjust(pval, method = "fdr"),
         significant = pval_adj < 0.05) %>%
  ungroup()

# ----------------------------------------------------------------------------
# Step 4: Identify Significant Regions
# ----------------------------------------------------------------------------

signif_regions_all <- diff_all %>%
  arrange(deliberation_function_1, deliberation_function_2, normalized_position) %>%
  group_by(deliberation_function_1, deliberation_function_2) %>%
  mutate(sig_grp = rleid(significant)) %>%
  filter(significant) %>%
  group_by(deliberation_function_1, deliberation_function_2, sig_grp) %>%
  summarise(
    xmin = min(normalized_position),
    xmax = max(normalized_position)
  ) %>%
  ungroup()

# ----------------------------------------------------------------------------
# Step 5: Plot Difference Curves with CI and Significance Bands
# ----------------------------------------------------------------------------

diff_plots <- list()

for(name in names(diff_dfs)) {
  
  df <- diff_dfs[[name]]
  
  sig_regions <- signif_regions_all %>%
    filter(deliberation_function_1 == unique(df$deliberation_function_1),
           deliberation_function_2 == unique(df$deliberation_function_2))
  
  p <- ggplot(df, aes(x = normalized_position, y = diff)) +
    
    # Significance Regions
    geom_rect(data = sig_regions,
              aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf),
              inherit.aes = FALSE,
              fill = "grey30", alpha = 0.1) +
    
    # Difference curve with CI
    geom_line(size = 1, color = "black") +
    geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci),
                fill = "grey70", alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    
    labs(
      title = paste0("Difference: ", unique(df$deliberation_function_1), 
                     " - ", unique(df$deliberation_function_2)),
      x = "Normalized Position in Verbalization",
      y = "Difference in Score"
    ) +
    theme_minimal(base_size = 14)
  
  diff_plots[[name]] <- p
}

p_diff_combined <- wrap_plots(diff_plots, ncol = 2)

p_diff_combined

# ----------------------------------------------------------------------------
# Step 6: Save Outputs
# ----------------------------------------------------------------------------

ggsave("./Output/gam_pairwise_functions_difference_trajectory.png",
       p_diff_combined, width = 14, height = 10, dpi = 600)
```

## By response

```{r}


# ----------------------------------------------------------------------------
# Fit GAM Model with Beta family and Random Effects
# ----------------------------------------------------------------------------
gam_model <- bam(
  score ~ response * deliberation_function + 
    s(normalized_position, by = interaction(response, deliberation_function), k = 10) +
    s(subject_id, bs = "re") +
    s(question, bs = "re"),
  data = chunks_long,
  method = "fREML"
)

summary(gam_model)

# ----------------------------------------------------------------------------
# Diagnostics
# ----------------------------------------------------------------------------
gam.check(gam_model)
appraise(gam_model)
```


```{r}
# ----------------------------------------------------------------------------
# Generate New Data for Predictions
# ----------------------------------------------------------------------------
newdata <- expand.grid(
  normalized_position = seq(0, 1, length.out = 100),
  deliberation_function = levels(chunks_long$deliberation_function),
  response = levels(chunks_long$response)
)


# Add dummy subject_id and question for prediction
newdata$subject_id <- levels(chunks_long$subject_id)[1]
newdata$question   <- levels(chunks_long$question)[1]

# Predict ignoring random effects
predictions <- predict(
  gam_model,
  newdata = newdata,
  se.fit = TRUE,
  exclude = c("s(subject_id)", "s(question)")
)

newdata$pred <- predictions$fit
newdata$se   <- predictions$se.fit


# ----------------------------------------------------------------------------
# Compute Pairwise Differences with z-test
# ----------------------------------------------------------------------------
newdata$diff <- NA
newdata$pval <- NA

for(f in unique(newdata$deliberation_function)) {
  for(pos in unique(newdata$normalized_position)) {
    
    tmp <- newdata %>%
      filter(deliberation_function == f, normalized_position == pos)
    
    d  <- tmp$pred[tmp$response == "Unbiased Response"] - tmp$pred[tmp$response == "Biased Response"]
    se <- sqrt(tmp$se[tmp$response == "Unbiased Response"]^2 + tmp$se[tmp$response == "Biased Response"]^2)
    
    zval <- d / se
    pval <- 2 * (1 - pnorm(abs(zval)))
    
    newdata$diff[newdata$deliberation_function == f & newdata$normalized_position == pos] <- d
    newdata$pval[newdata$deliberation_function == f & newdata$normalized_position == pos] <- pval
  }
}

# ----------------------------------------------------------------------------
# Multiple Comparisons Correction
# ----------------------------------------------------------------------------
newdata <- newdata %>%
  group_by(deliberation_function) %>%
  mutate(pval_adj = p.adjust(pval, method = "fdr"),
         significant = pval_adj < 0.05) %>%
  ungroup()
newdata$significant <- newdata$pval_adj < 0.05

# ----------------------------------------------------------------------------
# Compute Observed Means for Overlay
# ----------------------------------------------------------------------------
observed_means <- chunks_long %>%
  group_by(deliberation_function, response, normalized_position) %>%
  summarise(
    mean_score = mean(score, na.rm = TRUE),
    se_score   = sd(score, na.rm = TRUE) / sqrt(n())
  ) %>%
  ungroup()

# Identify significant contiguous regions
signif_regions <- newdata %>%
  arrange(deliberation_function, normalized_position) %>%
  group_by(deliberation_function) %>%
  mutate(sig_grp = rleid(significant)) %>%   # rleid = run-length encoding id
  filter(significant) %>%
  group_by(deliberation_function, sig_grp) %>%
  summarise(xmin = min(normalized_position),
            xmax = max(normalized_position)) %>%
  ungroup()



# ----------------------------------------------------------------------------
# Plot: Faceted Smooth Trajectories with Significance Zones
# ----------------------------------------------------------------------------
p_final <- ggplot() +
  
  # # Observed Means
  # geom_line(data = observed_means,
  #           aes(x = normalized_position, y = mean_score, color = response),
  #           size = 0.8) +
  
  # GAM Fit Lines
  geom_line(data = newdata,
            aes(x = normalized_position, y = pred, color = response),
            size = 1) +
  
  # GAM CI
  geom_ribbon(data = newdata,
              aes(x = normalized_position, ymin = pred - 1.96 * se, ymax = pred + 1.96 * se, fill = response),
              alpha = 0.2, color = NA) +
  
  # Clean Significance Bands
  geom_rect(data = signif_regions,
            aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf),
            fill = "grey30", alpha = 0.1) +
  
  facet_wrap(~deliberation_function, ncol = 2, scales = "free_y") +
  
  scale_color_manual(values = c("Unbiased Response" = "forestgreen",
                                "Biased Response"   = "tomato3")) +
  scale_fill_manual(values = c("Unbiased Response" = "forestgreen",
                               "Biased Response"   = "tomato3")) +
  
  theme_apa() +
  
  labs(x = "Normalized Position in Verbalization",
       y = "Predicted Score",
       color = "Response Type",
       fill  = "Response Type")

p_final
# ----------------------------------------------------------------------------
# Save Plot
# ----------------------------------------------------------------------------
ggsave("./Output/gam_trajectory_per_response_and_function.png",
       p_final, dpi = 600, width = 12, height = 8)
```

## Response and Lure Consideration

```{r}
# ----------------------------------------------------------------------------
# Recoding for clarity
# ----------------------------------------------------------------------------
chunks_long_lure <- chunks_long %>%
  mutate(
    lure_consideration = factor(lure_consideration, 
                                levels = c(0, 1), 
                                labels = c("Lure Non-Considered", "Lure Considered")),
    deliberation_function = factor(deliberation_function),
    response              = factor(response),
    subject_id            = factor(subject_id),
    question              = factor(question)
  ) %>% 
  filter(lure_consideration %in% c("Lure Non-Considered", "Lure Considered"),
         response %in% c("Unbiased Response", "Biased Response"))

# ----------------------------------------------------------------------------
# Model (as you did)
# ----------------------------------------------------------------------------
gam_model <- bam(
  score ~ lure_consideration * response * deliberation_function + 
    s(normalized_position, by = interaction(lure_consideration, response, deliberation_function), k = 10) +
    s(subject_id, bs = "re") +
    s(question, bs = "re"),
  data = chunks_long_lure,
  method = "fREML"
)

summary(gam_model)
gam.check(gam_model)
appraise(gam_model)

# ----------------------------------------------------------------------------
# Generate Prediction Data
# ----------------------------------------------------------------------------
newdata <- expand.grid(
  normalized_position = seq(0, 1, length.out = 100),
  deliberation_function = levels(chunks_long_lure$deliberation_function),
  response = levels(chunks_long_lure$response),
  lure_consideration = levels(chunks_long_lure$lure_consideration)
)

newdata$subject_id <- levels(chunks_long_lure$subject_id)[1]
newdata$question   <- levels(chunks_long_lure$question)[1]

predictions <- predict(
  gam_model,
  newdata = newdata,
  se.fit = TRUE,
  exclude = c("s(subject_id)", "s(question)")
)

newdata$pred <- predictions$fit
newdata$se   <- predictions$se.fit

# ----------------------------------------------------------------------------
# Compute Pairwise Differences: Lure vs No Lure
# ----------------------------------------------------------------------------
newdata$diff <- NA
newdata$pval <- NA

for(df in unique(newdata$deliberation_function)) {
  for(resp in unique(newdata$response)) {
    for(pos in unique(newdata$normalized_position)) {
      
      tmp <- newdata %>%
        filter(deliberation_function == df,
               response == resp,
               normalized_position == pos)
      
      d  <- tmp$pred[tmp$lure_consideration == "Lure Considered"] - 
            tmp$pred[tmp$lure_consideration == "Lure Non-Considered"]
      
      se <- sqrt(tmp$se[tmp$lure_consideration == "Lure Considered"]^2 +
                 tmp$se[tmp$lure_consideration == "Lure Non-Considered"]^2)
      
      zval <- d / se
      pval <- 2 * (1 - pnorm(abs(zval)))
      
      newdata$diff[newdata$deliberation_function == df & 
                   newdata$response == resp &
                   newdata$normalized_position == pos] <- d
      
      newdata$pval[newdata$deliberation_function == df & 
                   newdata$response == resp &
                   newdata$normalized_position == pos] <- pval
    }
  }
}

newdata <- newdata %>%
  group_by(deliberation_function, response) %>%
  mutate(pval_adj = p.adjust(pval, method = "fdr"),
         significant = pval_adj < 0.05) %>%
  ungroup()

newdata$significant <- newdata$pval_adj < 0.05

# ----------------------------------------------------------------------------
# Significant Regions
# ----------------------------------------------------------------------------
signif_regions <- newdata %>%
  arrange(deliberation_function, response, normalized_position) %>%
  group_by(deliberation_function, response) %>%
  mutate(sig_grp = data.table::rleid(significant)) %>%
  filter(significant) %>%
  group_by(deliberation_function, response, sig_grp) %>%
  summarise(xmin = min(normalized_position),
            xmax = max(normalized_position)) %>%
  ungroup()

# ----------------------------------------------------------------------------
# Plot
# ----------------------------------------------------------------------------
p_final <- ggplot() +
  
  geom_line(data = newdata,
            aes(x = normalized_position, y = pred, 
                color = lure_consideration),
            size = 1) +
  
  geom_ribbon(data = newdata,
              aes(x = normalized_position, ymin = pred - 1.96 * se, ymax = pred + 1.96 * se,
                  fill = lure_consideration),
              alpha = 0.2, color = NA) +
  
  geom_rect(data = signif_regions,
            aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf),
            fill = "grey30", alpha = 0.1) +
  
  facet_grid(response ~ deliberation_function, scales = "free_y") +
  
scale_color_manual(values = c("Lure Non-Considered" = "forestgreen",
                                "Lure Considered"   = "tomato3")) +
  scale_fill_manual(values = c("Lure Non-Considered" = "forestgreen",
                               "Lure Considered"   = "tomato3")) +
  
  theme_apa() +
  
  labs(x = "Normalized Position in Verbalization",
       y = "Predicted Score",
       color = "Lure Consideration",
       fill  = "Lure Consideration")

p_final

# ----------------------------------------------------------------------------
# Save
# ----------------------------------------------------------------------------
ggsave("./Output/gam_trajectory_lure_vs_no_lure_per_response_and_function.png",
       p_final, dpi = 600, width = 12, height = 8)

```

## Familiarity

```{r}
# ----------------------------------------------------------------------------
# Recoding for clarity
# ----------------------------------------------------------------------------
chunks_long_fam <- chunks_long %>%
  mutate(
    familiar = factor(familiar, levels = c(0,1), labels = c("Unfamiliar", "Familiar")),
    deliberation_function = factor(deliberation_function),
    subject_id = factor(subject_id),
    question   = factor(question)
  ) %>%
  filter(familiar %in% c("Unfamiliar", "Familiar"))

# ----------------------------------------------------------------------------
# Model
# ----------------------------------------------------------------------------
gam_model_fam <- bam(
  score ~ familiar * deliberation_function + 
    s(normalized_position, by = interaction(familiar, deliberation_function), k = 10) +
    s(subject_id, bs = "re") +
    s(question, bs = "re"),
  data = chunks_long_fam,
  method = "fREML"
)

summary(gam_model_fam)
gam.check(gam_model_fam)
appraise(gam_model_fam)

# ----------------------------------------------------------------------------
# Generate Prediction Data
# ----------------------------------------------------------------------------
newdata_fam <- expand.grid(
  normalized_position = seq(0, 1, length.out = 100),
  deliberation_function = levels(chunks_long_fam$deliberation_function),
  familiar = levels(chunks_long_fam$familiar)
)

newdata_fam$subject_id <- levels(chunks_long_fam$subject_id)[1]
newdata_fam$question   <- levels(chunks_long_fam$question)[1]

predictions_fam <- predict(
  gam_model_fam,
  newdata = newdata_fam,
  se.fit = TRUE,
  exclude = c("s(subject_id)", "s(question)")
)

newdata_fam$pred <- predictions_fam$fit
newdata_fam$se   <- predictions_fam$se.fit

# ----------------------------------------------------------------------------
# Compute Pairwise Differences: Familiar vs Unfamiliar
# ----------------------------------------------------------------------------
newdata_fam$diff <- NA
newdata_fam$pval <- NA

for(df in unique(newdata_fam$deliberation_function)) {
  for(pos in unique(newdata_fam$normalized_position)) {
    
    tmp <- newdata_fam %>%
      filter(deliberation_function == df,
             normalized_position == pos)
    
    d  <- tmp$pred[tmp$familiar == "Familiar"] - 
          tmp$pred[tmp$familiar == "Unfamiliar"]
    
    se <- sqrt(tmp$se[tmp$familiar == "Familiar"]^2 +
               tmp$se[tmp$familiar == "Unfamiliar"]^2)
    
    zval <- d / se
    pval <- 2 * (1 - pnorm(abs(zval)))
    
    newdata_fam$diff[newdata_fam$deliberation_function == df & 
                     newdata_fam$normalized_position == pos] <- d
    
    newdata_fam$pval[newdata_fam$deliberation_function == df & 
                     newdata_fam$normalized_position == pos] <- pval
  }
}

newdata_fam <- newdata_fam %>%
  group_by(deliberation_function) %>%
  mutate(pval_adj = p.adjust(pval, method = "fdr"),
         significant = pval_adj < 0.05) %>%
  ungroup()

# ----------------------------------------------------------------------------
# Significant Regions
# ----------------------------------------------------------------------------
signif_regions_fam <- newdata_fam %>%
  arrange(deliberation_function, normalized_position) %>%
  group_by(deliberation_function) %>%
  mutate(sig_grp = data.table::rleid(significant)) %>%
  filter(significant) %>%
  group_by(deliberation_function, sig_grp) %>%
  summarise(xmin = min(normalized_position),
            xmax = max(normalized_position)) %>%
  ungroup()

# ----------------------------------------------------------------------------
# Plot
# ----------------------------------------------------------------------------
p_fam <- ggplot() +
  
  geom_line(data = newdata_fam,
            aes(x = normalized_position, y = pred, 
                color = familiar),
            size = 1) +
  
  geom_ribbon(data = newdata_fam,
              aes(x = normalized_position, ymin = pred - 1.96 * se, ymax = pred + 1.96 * se,
                  fill = familiar),
              alpha = 0.2, color = NA) +
  
  geom_rect(data = signif_regions_fam,
            aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf),
            fill = "grey30", alpha = 0.1) +
  
  facet_wrap(~ deliberation_function, scales = "free_y") +
  
scale_color_manual(values = c("Unfamiliar" = "grey30",
                               "Familiar" = "steelblue")) +
scale_fill_manual(values = c("Unfamiliar" = "grey30",
                              "Familiar" = "steelblue")) +
  
  theme_apa() +
  
  labs(x = "Normalized Position in Verbalization",
       y = "Predicted Score",
       color = "Familiarity",
       fill  = "Familiarity")

p_fam

# ----------------------------------------------------------------------------
# Save
# ----------------------------------------------------------------------------
ggsave("./Output/gam_trajectory_familiar_vs_unfamiliar_per_function.png",
       p_fam, dpi = 600, width = 12, height = 6)
```




