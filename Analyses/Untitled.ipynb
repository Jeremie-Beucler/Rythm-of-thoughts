{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6b9578d-6ab9-46b1-92cb-92518f238727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 1. Imports and Config\n",
    "# --------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from huggingface_hub import InferenceClient\n",
    "import re\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 1\n",
    "MAX_TOKENS = 800\n",
    "N = 1\n",
    "\n",
    "data_long = pd.read_csv('../Data/data_long.csv')\n",
    "output_path = '../Output/llm_chunking_scoring.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaaafd8-e42d-430f-92be-410fe8b6aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_prompt = \"\"\"\n",
    "You are an expert in cognitive psychology and verbal protocol analysis.\n",
    "\n",
    "You are given a transcription of a participant thinking aloud while solving a problem.\n",
    "\n",
    "Your task is to segment this transcription into meaningful chunks.\n",
    "\n",
    "A chunk should correspond to a coherent idea, thought, or step in the participant's reasoning or verbal expression — including hesitations, repetitions, or meta-comments. The goal is not only to segment explicit reasoning steps but to preserve the full structure of the verbalization.\n",
    "\n",
    "Guidelines:\n",
    "- Do not remove or suppress any part of the original text.\n",
    "- Do not segment based on arbitrary word count or length.\n",
    "- Split only when the participant clearly moves to another distinct thought, idea, or reasoning step (e.g., shifting from generating an answer to justifying it, or reflecting on their uncertainty).\n",
    "- Be conservative in splitting: avoid unnecessary fragmentation.\n",
    "- Preserve the original wording exactly in each chunk.\n",
    "\n",
    "Provide your output strictly in the following structure:\n",
    "\n",
    "Chunk 1:\n",
    "[exact text of chunk 1]\n",
    "\n",
    "Chunk 2:\n",
    "[exact text of chunk 2]\n",
    "\n",
    "Chunk 3:\n",
    "[exact text of chunk 3]\n",
    "\n",
    "Be exhaustive.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63820f0f-3ff1-4ee9-ae18-feb1a3159840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1020 already chunked transcriptions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking transcriptions: 100%|██████████| 1020/1020 [00:00<00:00, 45663.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking completed and saved to ../Output/chunked_transcriptions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Define API Call\n",
    "# --------------------------------------------------------------------\n",
    "def chunk_transcription(transcription):\n",
    "    user_prompt = f\"Here is the transcription to chunk:\\n\\n{transcription}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLAMA_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": chunking_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Run Chunking Loop\n",
    "# --------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------\n",
    "# Run Chunking Loop (Skip Already Chunked)\n",
    "# --------------------------------------------------------------------\n",
    "output_path = '../Output/chunked_transcriptions.csv'\n",
    "os.makedirs('../Output', exist_ok=True)\n",
    "\n",
    "# Load existing results if available\n",
    "if os.path.exists(output_path):\n",
    "    existing_df = pd.read_csv(output_path)\n",
    "    already_chunked = set(zip(existing_df['subject_id'], existing_df['question']))\n",
    "    results = existing_df.to_dict(orient='records')\n",
    "    print(f\"Loaded {len(existing_df)} already chunked transcriptions.\")\n",
    "else:\n",
    "    already_chunked = set()\n",
    "    results = []\n",
    "\n",
    "# Loop over all transcriptions\n",
    "for idx, row in tqdm(data_long.iterrows(), total=len(data_long), desc=\"Chunking transcriptions\"):\n",
    "    key = (row['subject_id'], row['question'])\n",
    "\n",
    "    if key in already_chunked:\n",
    "        continue  # Skip already chunked\n",
    "\n",
    "    try:\n",
    "        chunks = chunk_transcription(row['transcription_new'])\n",
    "\n",
    "        results.append({\n",
    "            'subject_id': row['subject_id'],\n",
    "            'question': row['question'],\n",
    "            'transcription_new': row['transcription_new'],\n",
    "            'chunks': chunks\n",
    "        })\n",
    "\n",
    "        # Save every 20\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"Saving progress at idx {idx + 1}...\")\n",
    "            pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "        time.sleep(60)\n",
    "\n",
    "# Final save\n",
    "pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "print(f\"Chunking completed and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4ec6127-37dd-4a1a-be9d-ec1db8def1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoring_prompt = \"\"\"\n",
    "You are an expert in cognitive psychology.\n",
    "\n",
    "You are given a short chunk of a participant's think-aloud transcription during a reasoning task.\n",
    "\n",
    "Your task is to rate how strongly this chunk expresses each of the following deliberation functions.\n",
    "\n",
    "Definitions of the deliberation functions:\n",
    "\n",
    "- Response Control: Inhibiting, rejecting, or resisting an obvious or intuitive response that first comes to mind. Typical signs include expressions of doubt, suppression of initial answers, hesitation, or stopping oneself from blurting out an impulsive response.\n",
    "\n",
    "- Response Generation: Actively searching for new possible answers, alternatives, or hypotheses. This includes exploring options, mentally simulating scenarios, considering possibilities, or applying step-by-step logical reasoning.\n",
    "\n",
    "- Response Justification: Providing explicit reasons, arguments, or explanations to support a response that is currently being considered (whether intuitive or not). This includes defending a choice, explaining why an answer makes sense, or making an argument.\n",
    "\n",
    "- Response Regulation: Reflecting on one's own reasoning process, monitoring one's performance, allocating effort, expressing uncertainty, or deciding whether to continue thinking or stop. This includes metacognitive monitoring or strategic regulation of effort.\n",
    "\n",
    "Important Instructions:\n",
    "\n",
    "- Each score should reflect the extent to which the chunk expresses the function (even partially), using a continuous scale from 0 (not at all present) to 100 (very strongly present).\n",
    "- These functions are not mutually exclusive — a chunk may score highly on multiple functions if they co-occur.\n",
    "- If the chunk contains no trace of any of these 4 functions, assign 0 to all functions. This is perfectly acceptable.\n",
    "- If the chunk expresses a completely different kind of function (not captured by the 4 above), mention it below using a very broad and generic label (e.g., \"Reading Aloud\", \"Task Repetition\", \"Social Comment\", etc.). This should only happen rarely and only if clearly justified by the content of the chunk.\n",
    "- Be conservative: If you are unsure whether a function is expressed, prefer giving a low score (0-10).\n",
    "- Do not explain or justify the scores unless the chunk clearly expresses a different kind of function.\n",
    "\n",
    "Output strictly in this structure (and nothing else):\n",
    "\n",
    "Response Control: [score between 0 and 100]\n",
    "Response Generation: [score between 0 and 100]\n",
    "Response Justification: [score between 0 and 100]\n",
    "Response Regulation: [score between 0 and 100]\n",
    "\n",
    "[Optional broad label for a different function — only if clearly needed]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2d355-49ac-41d8-8226-f6c91a7950c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:   2%|▏         | 20/1020 [00:32<34:39,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 20...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Define API Call for Scoring\n",
    "# --------------------------------------------------------------------\n",
    "def score_chunk(chunk_text, previous_chunks, question_text, correct_answer, lured_answer):\n",
    "    context = \"\\n\\nPrevious chunks (for context):\\n\" + previous_chunks if previous_chunks else \"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Here is the question the participant was solving:\n",
    "\n",
    "{question_text}\n",
    "\n",
    "The most obvious or intuitive (but incorrect) answer is: {lured_answer}\n",
    "\n",
    "The correct answer is: {correct_answer}\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the current chunk of the transcription to score:\n",
    "\n",
    "{chunk_text}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLAMA_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": scoring_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reload Chunked Transcriptions\n",
    "# --------------------------------------------------------------------\n",
    "chunked_df = pd.read_csv('../Output/chunked_transcriptions.csv')\n",
    "\n",
    "chunked_df = chunked_df.merge(\n",
    "    data_long[['subject_id', 'question', 'question_text', 'correct_answer', 'lured_answer']],\n",
    "    on=['subject_id', 'question'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "output_path = '../Output/scored_chunks.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    existing_df = pd.read_csv(output_path)\n",
    "    already_scored = set(zip(existing_df['subject_id'], existing_df['question']))\n",
    "    results = existing_df.to_dict(orient='records')\n",
    "    print(f\"Loaded {len(existing_df)} already scored chunks.\")\n",
    "else:\n",
    "    already_scored = set()\n",
    "    results = []\n",
    "\n",
    "#chunked_df = chunked_df.head(10)  # For testing\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Main Scoring Loop\n",
    "# --------------------------------------------------------------------\n",
    "for idx, row in tqdm(chunked_df.iterrows(), total=len(chunked_df), desc=\"Scoring chunks\"):\n",
    "    key = (row['subject_id'], row['question'])\n",
    "\n",
    "    if key in already_scored:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chunks = row['chunks']\n",
    "        chunk_texts = re.split(r'Chunk \\d+:', chunks)\n",
    "        chunk_texts = [c.strip() for c in chunk_texts if c.strip()]\n",
    "\n",
    "        previous_chunks = \"\"\n",
    "\n",
    "        for chunk_id, chunk_text in enumerate(chunk_texts, start=1):\n",
    "\n",
    "            for attempt in range(3):\n",
    "                raw_output = score_chunk(\n",
    "                    chunk_text,\n",
    "                    previous_chunks,\n",
    "                    row['question_text'],\n",
    "                    row['correct_answer'],\n",
    "                    row['lured_answer']\n",
    "                )\n",
    "                scores, comment = parse_scores(raw_output)\n",
    "\n",
    "                if scores is not None:\n",
    "                    break\n",
    "                print(f\"Retry {attempt+1} for chunk {chunk_id}...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "            if scores is None:\n",
    "                scores = {\n",
    "                    'response_control': np.nan,\n",
    "                    'response_generation': np.nan,\n",
    "                    'response_justification': np.nan,\n",
    "                    'response_regulation': np.nan\n",
    "                }\n",
    "\n",
    "            results.append({\n",
    "                'subject_id': row['subject_id'],\n",
    "                'question': row['question'],\n",
    "                'chunk_id': chunk_id,\n",
    "                'chunk_text': chunk_text,\n",
    "                'response_control': scores['response_control'],\n",
    "                'response_generation': scores['response_generation'],\n",
    "                'response_justification': scores['response_justification'],\n",
    "                'response_regulation': scores['response_regulation'],\n",
    "                'llm_comment': comment\n",
    "            })\n",
    "\n",
    "            previous_chunks += chunk_text + \" \"  # Update context\n",
    "\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"Saving progress at idx {idx + 1}...\")\n",
    "            pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "        time.sleep(60)\n",
    "\n",
    "pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "print(f\"Scoring completed and saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
