{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9578d-6ab9-46b1-92cb-92518f238727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 1. Imports and Config\n",
    "# --------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from huggingface_hub import InferenceClient\n",
    "import re\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=\"REPLACE WITH YOUR KEY\",\n",
    ")\n",
    "\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 1\n",
    "MAX_TOKENS = 800\n",
    "N = 1\n",
    "\n",
    "data_long = pd.read_csv('../Data/data_long.csv')\n",
    "output_path = '../Output/llm_chunking_scoring.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaaafd8-e42d-430f-92be-410fe8b6aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_prompt = \"\"\"\n",
    "You are an expert in cognitive psychology and verbal protocol analysis.\n",
    "\n",
    "You are given a transcription of a participant thinking aloud while solving a problem.\n",
    "\n",
    "Your task is to segment this transcription into meaningful chunks.\n",
    "\n",
    "A chunk should correspond to a coherent idea, thought, or step in the participant's reasoning or verbal expression — including hesitations, repetitions, or meta-comments. The goal is not only to segment explicit reasoning steps but to preserve the full structure of the verbalization.\n",
    "\n",
    "Guidelines:\n",
    "- Do not remove or suppress any part of the original text.\n",
    "- Do not segment based on arbitrary word count or length.\n",
    "- Split only when the participant clearly moves to another distinct thought, idea, or reasoning step (e.g., shifting from generating an answer to justifying it, or reflecting on their uncertainty).\n",
    "- Be conservative in splitting: avoid unnecessary fragmentation.\n",
    "- Preserve the original wording exactly in each chunk.\n",
    "\n",
    "Provide your output strictly in the following structure:\n",
    "\n",
    "Chunk 1:\n",
    "[exact text of chunk 1]\n",
    "\n",
    "Chunk 2:\n",
    "[exact text of chunk 2]\n",
    "\n",
    "Chunk 3:\n",
    "[exact text of chunk 3]\n",
    "\n",
    "Be exhaustive.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63820f0f-3ff1-4ee9-ae18-feb1a3159840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1020 already chunked transcriptions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking transcriptions: 100%|██████████| 1020/1020 [00:00<00:00, 45663.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking completed and saved to ../Output/chunked_transcriptions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Define API Call\n",
    "# --------------------------------------------------------------------\n",
    "def chunk_transcription(transcription):\n",
    "    user_prompt = f\"Here is the transcription to chunk:\\n\\n{transcription}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLAMA_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": chunking_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Run Chunking Loop\n",
    "# --------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------\n",
    "# Run Chunking Loop (Skip Already Chunked)\n",
    "# --------------------------------------------------------------------\n",
    "output_path = '../Output/chunked_transcriptions.csv'\n",
    "os.makedirs('../Output', exist_ok=True)\n",
    "\n",
    "# Load existing results if available\n",
    "if os.path.exists(output_path):\n",
    "    existing_df = pd.read_csv(output_path)\n",
    "    already_chunked = set(zip(existing_df['subject_id'], existing_df['question']))\n",
    "    results = existing_df.to_dict(orient='records')\n",
    "    print(f\"Loaded {len(existing_df)} already chunked transcriptions.\")\n",
    "else:\n",
    "    already_chunked = set()\n",
    "    results = []\n",
    "\n",
    "# Loop over all transcriptions\n",
    "for idx, row in tqdm(data_long.iterrows(), total=len(data_long), desc=\"Chunking transcriptions\"):\n",
    "    key = (row['subject_id'], row['question'])\n",
    "\n",
    "    if key in already_chunked:\n",
    "        continue  # Skip already chunked\n",
    "\n",
    "    try:\n",
    "        chunks = chunk_transcription(row['transcription_new'])\n",
    "\n",
    "        results.append({\n",
    "            'subject_id': row['subject_id'],\n",
    "            'question': row['question'],\n",
    "            'transcription_new': row['transcription_new'],\n",
    "            'chunks': chunks\n",
    "        })\n",
    "\n",
    "        # Save every 20\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"Saving progress at idx {idx + 1}...\")\n",
    "            pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "        time.sleep(60)\n",
    "\n",
    "# Final save\n",
    "pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "print(f\"Chunking completed and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4ec6127-37dd-4a1a-be9d-ec1db8def1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoring_prompt = \"\"\"\n",
    "You are an expert in cognitive psychology.\n",
    "\n",
    "You are given a short chunk of a participant's think-aloud transcription during a reasoning task.\n",
    "\n",
    "Your task is to rate how strongly this chunk expresses each of the following deliberation functions.\n",
    "\n",
    "Definitions of the deliberation functions:\n",
    "\n",
    "- Response Control: Inhibiting, rejecting, or resisting an obvious or intuitive response that first comes to mind. Typical signs include expressions of doubt, suppression of initial answers, hesitation, or stopping oneself from blurting out an impulsive response.\n",
    "\n",
    "- Response Generation: Actively searching for new possible answers, alternatives, or hypotheses. This includes exploring options, mentally simulating scenarios, considering possibilities, or applying step-by-step logical reasoning.\n",
    "\n",
    "- Response Justification: Providing explicit reasons, arguments, or explanations to support a response that is currently being considered (whether intuitive or not). This includes defending a choice, explaining why an answer makes sense, or making an argument.\n",
    "\n",
    "- Response Regulation: Reflecting on one's own reasoning process, monitoring one's performance, allocating effort, expressing uncertainty, or deciding whether to continue thinking or stop. This includes metacognitive monitoring or strategic regulation of effort.\n",
    "\n",
    "Important Instructions:\n",
    "\n",
    "- Each score should reflect the extent to which the chunk expresses the function (even partially), using a continuous scale from 0 (not at all present) to 100 (very strongly present).\n",
    "- These functions are not mutually exclusive — a chunk may score highly on multiple functions if they co-occur.\n",
    "- If the chunk contains no trace of any of these 4 functions, assign 0 to all functions. This is perfectly acceptable.\n",
    "- If the chunk expresses a completely different kind of function (not captured by the 4 above), mention it below using a very broad and generic label (e.g., \"Reading Aloud\", \"Task Repetition\", \"Social Comment\", etc.). This should only happen rarely and only if clearly justified by the content of the chunk.\n",
    "- Be conservative: If you are unsure whether a function is expressed, prefer giving a low score (0-10).\n",
    "- Do not explain or justify the scores unless the chunk clearly expresses a different kind of function.\n",
    "\n",
    "Output strictly in this structure (and nothing else):\n",
    "\n",
    "Response Control: [score between 0 and 100]\n",
    "Response Generation: [score between 0 and 100]\n",
    "Response Justification: [score between 0 and 100]\n",
    "Response Regulation: [score between 0 and 100]\n",
    "\n",
    "[Optional broad label for a different function — only if clearly needed]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01f2d355-49ac-41d8-8226-f6c91a7950c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:   2%|▏         | 20/1020 [00:32<34:39,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:   4%|▍         | 40/1020 [01:38<41:51,  2.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:   6%|▌         | 60/1020 [02:32<33:49,  2.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:   8%|▊         | 80/1020 [03:11<39:06,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 80...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  10%|▉         | 100/1020 [04:06<26:36,  1.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  12%|█▏        | 120/1020 [05:04<32:44,  2.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  14%|█▎        | 140/1020 [05:25<07:51,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 140...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  16%|█▌        | 160/1020 [06:11<09:00,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  18%|█▊        | 180/1020 [07:09<34:30,  2.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 180...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  20%|█▉        | 200/1020 [08:14<58:48,  4.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  22%|██▏       | 220/1020 [09:26<39:26,  2.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 220...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  24%|██▎       | 240/1020 [09:44<08:19,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 240...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  25%|██▌       | 260/1020 [10:17<30:19,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 260...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  27%|██▋       | 280/1020 [10:56<25:49,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 280...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  29%|██▉       | 300/1020 [11:54<23:38,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  31%|███▏      | 320/1020 [13:10<35:15,  3.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 320...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  33%|███▎      | 340/1020 [14:59<46:05,  4.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 340...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  35%|███▌      | 360/1020 [16:36<44:20,  4.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 360...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  37%|███▋      | 380/1020 [17:48<47:10,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 380...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  39%|███▉      | 400/1020 [19:01<31:21,  3.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 400...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  41%|████      | 420/1020 [20:28<1:14:05,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 420...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  43%|████▎     | 440/1020 [21:04<07:54,  1.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 440...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  45%|████▌     | 460/1020 [21:48<41:11,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 460...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  47%|████▋     | 480/1020 [23:08<28:28,  3.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 480...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  49%|████▉     | 500/1020 [24:33<23:02,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  51%|█████     | 520/1020 [25:25<21:45,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 520...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  53%|█████▎    | 540/1020 [26:11<16:38,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 540...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  55%|█████▍    | 560/1020 [27:04<22:09,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 560...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  57%|█████▋    | 580/1020 [28:08<09:49,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 580...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  59%|█████▉    | 600/1020 [29:23<27:42,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 600...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  61%|██████    | 620/1020 [30:57<28:22,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 620...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  63%|██████▎   | 640/1020 [32:17<20:39,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 640...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  65%|██████▍   | 660/1020 [33:49<23:02,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 660...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  67%|██████▋   | 680/1020 [34:44<13:22,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 680...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  69%|██████▊   | 700/1020 [36:10<19:43,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 700...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  71%|███████   | 720/1020 [37:54<22:01,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 720...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  73%|███████▎  | 740/1020 [38:28<04:02,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 740...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  75%|███████▍  | 760/1020 [39:28<15:31,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 760...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  76%|███████▋  | 780/1020 [40:46<09:23,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 780...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  78%|███████▊  | 800/1020 [41:49<13:31,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 800...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  80%|████████  | 820/1020 [42:38<14:57,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 820...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  82%|████████▏ | 840/1020 [43:41<05:04,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 840...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  84%|████████▍ | 860/1020 [44:52<06:18,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 860...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  86%|████████▋ | 880/1020 [46:30<18:49,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 880...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  88%|████████▊ | 900/1020 [47:21<04:28,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 900...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  90%|█████████ | 920/1020 [48:06<01:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 920...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  92%|█████████▏| 940/1020 [48:48<01:31,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 940...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  94%|█████████▍| 960/1020 [49:45<02:19,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 960...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  96%|█████████▌| 980/1020 [50:43<03:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 980...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks:  98%|█████████▊| 1000/1020 [51:49<00:54,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring chunks: 100%|██████████| 1020/1020 [53:08<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving progress at idx 1020...\n",
      "Scoring completed and saved to ../Output/scored_chunks.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Define API Call for Scoring\n",
    "# --------------------------------------------------------------------\n",
    "def score_chunk(chunk_text, previous_chunks, question_text, correct_answer, lured_answer):\n",
    "    context = \"\\n\\nPrevious chunks (for context):\\n\" + previous_chunks if previous_chunks else \"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Here is the question the participant was solving:\n",
    "\n",
    "{question_text}\n",
    "\n",
    "The most obvious or intuitive (but incorrect) answer is: {lured_answer}\n",
    "\n",
    "The correct answer is: {correct_answer}\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the current chunk of the transcription to score:\n",
    "\n",
    "{chunk_text}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLAMA_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": scoring_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Reload Chunked Transcriptions\n",
    "# --------------------------------------------------------------------\n",
    "chunked_df = pd.read_csv('../Output/chunked_transcriptions.csv')\n",
    "\n",
    "chunked_df = chunked_df.merge(\n",
    "    data_long[['subject_id', 'question', 'question_text', 'correct_answer', 'lured_answer']],\n",
    "    on=['subject_id', 'question'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "output_path = '../Output/scored_chunks.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    existing_df = pd.read_csv(output_path)\n",
    "    already_scored = set(zip(existing_df['subject_id'], existing_df['question']))\n",
    "    results = existing_df.to_dict(orient='records')\n",
    "    print(f\"Loaded {len(existing_df)} already scored chunks.\")\n",
    "else:\n",
    "    already_scored = set()\n",
    "    results = []\n",
    "\n",
    "#chunked_df = chunked_df.head(10)  # For testing\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Main Scoring Loop\n",
    "# --------------------------------------------------------------------\n",
    "for idx, row in tqdm(chunked_df.iterrows(), total=len(chunked_df), desc=\"Scoring chunks\"):\n",
    "    key = (row['subject_id'], row['question'])\n",
    "\n",
    "    if key in already_scored:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        chunks = row['chunks']\n",
    "        chunk_texts = re.split(r'Chunk \\d+:', chunks)\n",
    "        chunk_texts = [c.strip() for c in chunk_texts if c.strip()]\n",
    "\n",
    "        previous_chunks = \"\"\n",
    "\n",
    "        for chunk_id, chunk_text in enumerate(chunk_texts, start=1):\n",
    "\n",
    "            for attempt in range(3):\n",
    "                raw_output = score_chunk(\n",
    "                    chunk_text,\n",
    "                    previous_chunks,\n",
    "                    row['question_text'],\n",
    "                    row['correct_answer'],\n",
    "                    row['lured_answer']\n",
    "                )\n",
    "                scores, comment = parse_scores(raw_output)\n",
    "\n",
    "                if scores is not None:\n",
    "                    break\n",
    "                print(f\"Retry {attempt+1} for chunk {chunk_id}...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "            if scores is None:\n",
    "                scores = {\n",
    "                    'response_control': np.nan,\n",
    "                    'response_generation': np.nan,\n",
    "                    'response_justification': np.nan,\n",
    "                    'response_regulation': np.nan\n",
    "                }\n",
    "\n",
    "            results.append({\n",
    "                'subject_id': row['subject_id'],\n",
    "                'question': row['question'],\n",
    "                'chunk_id': chunk_id,\n",
    "                'chunk_text': chunk_text,\n",
    "                'response_control': scores['response_control'],\n",
    "                'response_generation': scores['response_generation'],\n",
    "                'response_justification': scores['response_justification'],\n",
    "                'response_regulation': scores['response_regulation'],\n",
    "                'llm_comment': comment\n",
    "            })\n",
    "\n",
    "            previous_chunks += chunk_text + \" \"  # Update context\n",
    "\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"Saving progress at idx {idx + 1}...\")\n",
    "            pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "        time.sleep(60)\n",
    "\n",
    "pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "print(f\"Scoring completed and saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
